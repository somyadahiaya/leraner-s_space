{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10ba53bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49d99487",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top‑10 next‑token probabilities:\n",
      " 1. 'Ġa'             p=0.1279\n",
      " 2. 'Ġan'            p=0.0453\n",
      " 3. 'Ġthe'           p=0.0272\n",
      " 4. 'Ġanti'          p=0.0269\n",
      " 5. 'Ġintraven'      p=0.0189\n",
      " 6. 'Ġantibiotics'   p=0.0178\n",
      " 7. 'Ġanalges'       p=0.0131\n",
      " 8. 'Ġmorphine'      p=0.0118\n",
      " 9. 'Ġant'           p=0.0115\n",
      "10. 'Ġoxygen'        p=0.0111\n",
      "\n",
      "Greedy continuation:\n",
      "The patient was treated with a combination of antibiotics and a topical steroid . \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ── 0. Install deps once (skip if already done) ─────────────────────────\n",
    "# !pip install transformers torch --upgrade\n",
    "\n",
    "# ── 1. Load model & tokenizer ──────────────────────────────────────────\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, set_seed\n",
    "import torch, math\n",
    "\n",
    "MODEL_DIR = \"./wikitext_finetuned\"       # ← adjust if you moved the folder\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR)\n",
    "model     = AutoModelForCausalLM.from_pretrained(MODEL_DIR)\n",
    "device    = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device).eval()\n",
    "\n",
    "# Ensure pad_token_id is set (needed for GPT‑2 during generation)\n",
    "if model.config.pad_token_id is None:\n",
    "    model.config.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# ── 2. Helper: show top‑k next tokens ──────────────────────────────────\n",
    "def next_token_ranking(prompt, k=10):\n",
    "    enc = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        logits = model(**enc).logits\n",
    "    probs = torch.softmax(logits[0, -1], dim=-1)\n",
    "    top = torch.topk(probs, k)\n",
    "    tokens = tokenizer.convert_ids_to_tokens(top.indices.tolist())\n",
    "    for i, (tok, p) in enumerate(zip(tokens, top.values.tolist()), 1):\n",
    "        print(f\"{i:2d}. {tok!r:15}  p={p:.4f}\")\n",
    "\n",
    "# ── 3. Helper: generate continuation ───────────────────────────────────\n",
    "def generate(prompt, max_new_tokens=20, do_sample=False, **kw):\n",
    "    enc = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    out_ids = model.generate(\n",
    "        **enc,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=do_sample,   # False = greedy; True = sampling\n",
    "        top_p=0.95,\n",
    "        temperature=1.0,\n",
    "        no_repeat_ngram_size=2,\n",
    "        **kw\n",
    "    )\n",
    "    return tokenizer.decode(out_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# ── 4. Sanity checks ───────────────────────────────────────────────────\n",
    "prompt = \"The patient was treated with\"\n",
    "\n",
    "print(\"Top‑10 next‑token probabilities:\")\n",
    "next_token_ranking(prompt, k=10)\n",
    "\n",
    "print(\"\\nGreedy continuation:\")\n",
    "print(generate(prompt, max_new_tokens=10, do_sample=False))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "810234ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Robert Boulter is an\n",
      "Top‑10 next‑token probabilities:\n",
      " 1. 'ĠAmerican'      p=0.0992\n",
      " 2. 'Ġassociate'     p=0.0959\n",
      " 3. 'Ġaward'         p=0.0708\n",
      " 4. 'Ġassistant'     p=0.0454\n",
      " 5. 'Ġauthor'        p=0.0409\n",
      " 6. 'ĠAustralian'    p=0.0359\n",
      " 7. 'Ġexecutive'     p=0.0277\n",
      " 8. 'ĠEnglish'       p=0.0261\n",
      " 9. 'Ġindependent'   p=0.0205\n",
      "10. 'Ġactor'         p=0.0203\n"
     ]
    }
   ],
   "source": [
    "prompt='Robert Boulter is an'\n",
    "print(prompt)\n",
    "print(\"Top‑10 next‑token probabilities:\")\n",
    "next_token_ranking(prompt, k=10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
