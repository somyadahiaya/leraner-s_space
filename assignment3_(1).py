# -*- coding: utf-8 -*-
"""Assignment3 (1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1w34hukHr4jprakmTpDk5f2omfRDJy5bR
"""

# !pip install --upgrade transformers
# !pip install torch
# !pip install fsspec==2023.6.0

# !pip install datasets

from datasets import load_dataset

data = load_dataset("imdb")

print(data)

from transformers import BertTokenizer

tokenizer=BertTokenizer.from_pretrained("bert-base-uncased")

def tokenize(example):
  return tokenizer(example["text"],padding="max_length",truncation=True,max_length=128)

tokenized_data=data.map(tokenize,batched=True)


print(tokenized_data)

train_testvalid = tokenized_data['train'].train_test_split(test_size=0.2)
train_dataset = train_testvalid['train']
valid_dataset = train_testvalid['test']

from torch.utils.data import DataLoader
train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=8)
valid_dataloader = DataLoader(valid_dataset, batch_size=8)

from transformers import BertForSequenceClassification, Trainer, TrainingArguments
from torch.optim import AdamW
model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)

import os
os.environ["WANDB_DISABLED"] = "true"

import numpy as np

from sklearn.metrics import accuracy_score, f1_score   # !pip install scikit-learn if missing

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    preds = np.argmax(logits, axis=-1)
    acc = accuracy_score(labels, preds)
    f1  = f1_score(labels, preds, average="weighted")
    return {"accuracy": acc, "f1": f1}

!pip install 'accelerate>=0.26.0'
!pip install transformers[torch]

from transformers import TrainingArguments, Trainer

# Define training arguments
training_args = TrainingArguments(
output_dir='./results',
do_eval=True,
learning_rate=2e-5,
per_device_train_batch_size=8,
per_device_eval_batch_size=8,
num_train_epochs=3,
weight_decay=0.01,
)
# Define Trainer with model, arguments, and datasets
trainer = Trainer(
model=model,
args=training_args,
train_dataset=train_dataset,
eval_dataset=valid_dataset,
compute_metrics=compute_metrics
)
# Start training
trainer.train()

metrics = trainer.evaluate()
print(metrics)

predictions = trainer.predict(valid_dataset)
print(predictions)

import torch

print(f"Accuracy: {metrics['eval_accuracy']:.4f}")
print(f"F1 score: {metrics['eval_f1']:.4f}")

# ---------- inference on custom text -----------------


def predict_sentiment(text):
    inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True, max_length=128)
    inputs = {k: v.to(model.device) for k, v in inputs.items()}  # Move input to model's device
    with torch.no_grad():
        outputs = model(**inputs)
        logits = outputs.logits
        pred = torch.argmax(logits, dim=1).item()
    return "positive" if pred == 1 else "negative"


print(predict_sentiment("very good movie"))  
print(predict_sentiment("that was terrible"))  
print(predict_sentiment("the movie was fine but the acting was not upto the mark"))

# -----------------------------------------------------



#  "SillySom/results" is the final model which i have uploaded on the hugging face 
